{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "YvR_tMPdmf4X"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**テーマ**　：　迷路の最短ルートを導き出したい\n",
        "\n",
        "**基礎となるアルゴリズム**　：　迷路作成、報酬の設定、Q学習を用いた強化学習、最短ルートの表示\n",
        "\n",
        "**実行プログラム、その結果**　：　下記\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GWWqKtOB7LFx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**今後自分の研究を深める上で必要なツール**　：　道の最短経路を用いるためQ学習を使うと考えられる。\n",
        "\n",
        "\n",
        "**Numpy、Pandas、Matplotlibといった講義で説明したモジュールを利用していること**　：　Numpyを使用\n",
        "\n",
        "\n",
        "**何らかのデータを入力、出力すること**　：　迷路のデータ、迷路の報酬の設定、最短経路を作成し出力している"
      ],
      "metadata": {
        "id": "5tjR33ty8AT5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q学習**\n",
        "[参考先](https://note.com/pumonmon/n/n04f9139ad826)"
      ],
      "metadata": {
        "id": "YvR_tMPdmf4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_action(state, action, observation, reward):\n",
        "   next_state = digitize_state(observation)\n",
        "   next_action = np.argmax(q_table[next_state])\n",
        "\n",
        "   alpha = 0.2\n",
        "   gamma = 0.99\n",
        "   q_table[state, action] = (1 - alpha) * q_table[state, action] +\\\n",
        "           alpha * (reward + gamma * q_table[next_state, next_action])\n",
        "   return next_action, next_state\n",
        "for episode in range(num_episodes):\n",
        "\n",
        "   observation = env.reset()\n",
        "   state = digitize_state(observation)\n",
        "   action = np.argmax(q_table[state])\n",
        "   episode_reward = 0\n",
        "   for t in range(max_number_of_steps):\n",
        "\n",
        "       env.render()\n",
        "\n",
        "       observation, reward, done, info = env.step(action)\n",
        "\n",
        "       action, state = get_action(state, action, observation, reward)\n",
        "       episode_reward += reward"
      ],
      "metadata": {
        "id": "tx15_YDMdapk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **強化学習の報酬の基準**\n",
        "\n",
        "進む先が正しいときは報酬の１を渡す。\n",
        "\n",
        "進先＜ゴールとなるようにゴールの報酬を大きくする。\n",
        "\n",
        "迷路を大きくしたいが手作業でrewardを作るのはめんどくさいのでまずはその部分のプログラムを作る。\n",
        "\n",
        "追記:ランダムで迷路を作成するものも欲しくなったため、それも作成。順番は前後した。"
      ],
      "metadata": {
        "id": "boQAh1UJoGsm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **迷路を作る**"
      ],
      "metadata": {
        "id": "T7YbRqIH_HDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# 迷路の大きさ\n",
        "maze_size = [5,5]\n",
        "\n",
        "# スタートとゴールの位置\n",
        "start = (0, 0)\n",
        "goal =list(map(lambda x: x - 1, maze_size))\n",
        "\n",
        "def make_maze(maze_size, start, goal):\n",
        "    # 迷路を全て壁（0）で初期化\n",
        "    maze = np.zeros(maze_size, dtype=int)\n",
        "\n",
        "    # スタート地点を1にする\n",
        "    maze[start] = 1\n",
        "\n",
        "    # 方向ベクトル（上下左右）\n",
        "    directions = [(2, 0), (-2, 0), (0, 2), (0, -2)]\n",
        "\n",
        "    def is_valid_move(x, y):\n",
        "        # 迷路の範囲内かチェック\n",
        "        return 0 <= x < maze_size[0] and 0 <= y < maze_size[1]\n",
        "\n",
        "    def carve_passage_from(x, y):\n",
        "        # 方向をランダムにシャッフル\n",
        "        random.shuffle(directions)\n",
        "        for dx, dy in directions:\n",
        "            nx, ny = x + dx, y + dy\n",
        "            if is_valid_move(nx, ny) and maze[nx, ny] == 0 :\n",
        "                # 壁を壊して道を作る\n",
        "                maze[nx, ny] = 1\n",
        "                maze[x + dx // 2, y + dy // 2] = 1\n",
        "                carve_passage_from(nx, ny)\n",
        "\n",
        "    # スタート地点から穴を掘る\n",
        "    carve_passage_from(*start)\n",
        "\n",
        "    # ゴール地点を道にする\n",
        "    maze[goal] = 1\n",
        "\n",
        "    return maze\n",
        "\n",
        "maze = make_maze(maze_size, start, goal)\n",
        "print(maze)\n",
        "[0, 1, 2, 7, 12, 17, 22, 23, 24]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f1bMaS2GtgU",
        "outputId": "b83fa609-5fb3-4ba3-a8a7-b1c2a9084463"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 1 1 0 1]\n",
            " [0 0 1 0 1]\n",
            " [1 1 1 0 1]\n",
            " [1 0 1 0 1]\n",
            " [1 1 1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **迷路を報酬つきにする**"
      ],
      "metadata": {
        "id": "3JuhaFl18D5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 迷路の定義（スタートは0、ゴールは8）\n",
        "# 1は通れる道、0は壁\n",
        "#maze_sizeの迷路を0,1で作成\n",
        "\n",
        "\"\"\"\n",
        "迷路作成時に設定済\n",
        "\n",
        "# 迷路の大きさ\n",
        "maze_size = (5, 5)\n",
        "\n",
        "# スタートとゴールの位置\n",
        "start = (0, 0)\n",
        "goal = (2, 2)\n",
        "\n",
        "#スタートとゴールの位置を1にする\n",
        "maze[start] = 1\n",
        "maze[goal] = 1\n",
        "\n",
        "#mazeが機能しているかの確認\n",
        "print(maze)\n",
        "\"\"\"\n",
        "\n",
        "#迷路のreward化(通れるかどうかを判定)\n",
        "def make_reward(maze):\n",
        "    size = maze.shape[0] * maze.shape[1]\n",
        "    reward = np.zeros((size, size), dtype = int)\n",
        "\n",
        "    #上下左右に1があれば1を置く\n",
        "    for i in range(maze.shape[0]):\n",
        "        for j in range(maze.shape[1]):\n",
        "            if maze[i, j] == 1:\n",
        "                current_state = i * maze.shape[1] + j\n",
        "                if i > 0 and maze[i-1, j] == 1:  # 上に移動可能\n",
        "                    next_state = (i-1) * maze.shape[1] + j\n",
        "                    reward[current_state, next_state] = 1\n",
        "                if i < maze.shape[0] - 1 and maze[i+1, j] == 1:  # 下に移動可能\n",
        "                    next_state = (i+1) * maze.shape[1] + j\n",
        "                    reward[current_state, next_state] = 1\n",
        "                if j > 0 and maze[i, j-1] == 1:  # 左に移動可能\n",
        "                    next_state = i * maze.shape[1] + (j-1)\n",
        "                    reward[current_state, next_state] = 1\n",
        "                if j < maze.shape[1] - 1 and maze[i, j+1] == 1:  # 右に移動可能\n",
        "                    next_state = i * maze.shape[1] + (j+1)\n",
        "                    reward[current_state, next_state] = 1\n",
        "\n",
        "\n",
        "    #ゴールの報酬部分の入れ替え\n",
        "    for i in range(reward.shape[0]):\n",
        "      if reward[i, (reward.shape[0] - 1)] == 1:\n",
        "        reward[i, (reward.shape[0] - 1)] = 100000\n",
        "\n",
        "    return reward\n",
        "\n",
        "\n",
        "# reward行列の生成\n",
        "reward = make_reward(maze)\n",
        "\n",
        "#rewardの確認\n",
        "print(reward)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96fo_ScqnrJL",
        "outputId": "f4288813-e3c2-473c-b08d-3ee73c0e6963"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[     0      1      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [     1      0      1      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [     0      1      0      0      0      0      0      1      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [     0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [     0      0      0      0      0      0      0      0      0      1\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [     0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [     0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [     0      0      1      0      0      0      0      0      0      0\n",
            "       0      0      1      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [     0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [     0      0      0      0      1      0      0      0      0      0\n",
            "       0      0      0      0      1      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [     0      0      0      0      0      0      0      0      0      0\n",
            "       0      1      0      0      0      1      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [     0      0      0      0      0      0      0      0      0      0\n",
            "       1      0      1      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [     0      0      0      0      0      0      0      1      0      0\n",
            "       0      1      0      0      0      0      0      1      0      0\n",
            "       0      0      0      0      0]\n",
            " [     0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [     0      0      0      0      0      0      0      0      0      1\n",
            "       0      0      0      0      0      0      0      0      0      1\n",
            "       0      0      0      0      0]\n",
            " [     0      0      0      0      0      0      0      0      0      0\n",
            "       1      0      0      0      0      0      0      0      0      0\n",
            "       1      0      0      0      0]\n",
            " [     0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [     0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      1      0      0      0      0      0      0      0\n",
            "       0      0      1      0      0]\n",
            " [     0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [     0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      1      0      0      0      0      0\n",
            "       0      0      0      0 100000]\n",
            " [     0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      1      0      0      0      0\n",
            "       0      1      0      0      0]\n",
            " [     0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       1      0      1      0      0]\n",
            " [     0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      1      0      0\n",
            "       0      1      0      1      0]\n",
            " [     0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      1      0 100000]\n",
            " [     0      0      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0      0      0      0      0      1\n",
            "       0      0      0      1      0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **最終テスト時**"
      ],
      "metadata": {
        "id": "Ekv3M9be-JN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "gamma = 0.9 #小さいと行動直後の利益を重視。\n",
        "alpha = 0.1 #学習率\n",
        "num = 10000 #何回行うか\n",
        "\n",
        "#Q値(行動価値)の初期値を設定\n",
        "Q = np.zeros((maze_size[0] * maze_size[1], maze_size[0] * maze_size[1]))\n",
        "\n",
        "#Q学習を実装し、各位置における行動価値を算出\n",
        "for i in range(num):#numの回数繰り返し学習を行う\n",
        "    p_state = np.random.randint(0, maze_size[0] * maze_size[1])#現在の状態をランダムに選択\n",
        "    n_actions = []#次の行動の候補\n",
        "    for j in range(maze_size[0] * maze_size[1]):\n",
        "        if reward[p_state,j] >= 1:\n",
        "            n_actions.append(j) #p_stateの状態で移動できる場所を取得\n",
        "    if len(n_actions) > 0:\n",
        "        n_state = np.random.choice(n_actions)\n",
        "        Q[p_state, n_state] = (1 - alpha) * Q[p_state, n_state] + alpha * (reward[p_state, n_state] + gamma * Q[n_state, np.argmax(Q[n_state, :])])\n",
        "    else:\n",
        "        # p_stateから移動可能なアクションがない場合\n",
        "        continue\n",
        "\n",
        "    #Q値の更新\n",
        "    Q[p_state,n_state] = (1-alpha)*Q[p_state,n_state]+alpha*(reward[p_state,n_state]+gamma*Q[n_state,np.argmax(Q[n_state,])])\n",
        "#最短ルート表示関数の定義。Q値が最も高い行動をappendで追加していく\n",
        "def shortest_path(start):#startに依存する\n",
        "    path = [start]#pathに経路を追加していく\n",
        "    p_pos = start#p_posは現在位置\n",
        "    n_pos = p_pos#n_posにいったんp_posを代入\n",
        "    while(n_pos != (maze_size[0]**2 - 1)):#n_posがゴールになるまで繰り返し行動を選択\n",
        "        n_pos = np.argmax(Q[p_pos,])#各位置の行動価値が最も高い行動を選択\n",
        "        path.append(n_pos)#経路をpathに追加\n",
        "        p_pos = n_pos#行動後が次のp_posとなる\n",
        "    return path\n",
        "print(shortest_path(0))#スタートを0として最短経路を表示"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IU-QYa4E8cew",
        "outputId": "f9a51346-b1f3-4e85-898a-1601f701392f"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 7, 12, 17, 22, 23, 24]\n"
          ]
        }
      ]
    }
  ]
}